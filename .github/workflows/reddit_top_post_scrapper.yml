name: Reddit scrapper
on:
  schedule:
    - cron: "0 0 * * *"
jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: checkout repo content
        uses: actions/checkout@v2 # checkout the repository content to github runner.
      - name: setup python
        uses: actions/setup-python@v2
        with:
          python-version: 3.8 #install the python needed
      - name: s3 download
        uses: keithweaver/aws-s3-github-action@v1.0.0
        with:
          command: cp
          source: s3://reddit-scraper-top-posts-raghava-akkul/reddit_top_posts.csv
          destination: '/home/runner/work/reddit_scrapper/reddit_scrapper/reddit/spiders/csv/reddit_downloaded.csv'
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws_region: ap-south-1
      - name: execute py script # run the run.py to get the latest data
        run: |
          pip install virtualenv
          virtualenv venv 
          source venv/bin/activate
          pip install -r requirements.txt
          cd reddit/spiders
          scrapy crawl post -s CLOSESPIDER_ITEMCOUNT=1000 -o posts.json
          sudo chmod -R 777 csv
          sudo apt install csvkit
          python json_parser.py
          cd csv
          csvstack reddit_downloaded.csv reddit_top_posts.csv > reddit_top_posts_s3.csv
          rm reddit_downloaded.csv reddit_top_posts.csv
          ls

      - name: s3 upload
        uses: keithweaver/aws-s3-github-action@v1.0.0
        with:
          command: cp
          source: '/home/runner/work/reddit_scrapper/reddit_scrapper/reddit/spiders/csv/reddit_top_posts_s3.csv'
          destination: s3://reddit-scraper-top-posts-raghava-akkul/reddit_top_posts.csv       
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws_region: ap-south-1
