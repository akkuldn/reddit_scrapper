name: Reddit scrapper
on:
  push:
    branches:
      - main
jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: checkout repo content
        uses: actions/checkout@v2 # checkout the repository content to github runner.
      - name: setup python
        uses: actions/setup-python@v2
        with:
          python-version: 3.8 #install the python needed
      - name: s3 download
        uses: keithweaver/aws-s3-github-action@v1.0.0
        with:
          command: cp
          source: s3://reddit-scraper-top-posts-raghava-akkul/reddit_top_posts.csv
          destination: '/home/runner/work/reddit_scrapper/reddit_scrapper/reddit/spiders/csv/reddit_downloaded.csv'
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws_region: ap-south-1
      - name: execute py script # run the run.py to get the latest data
        run: |
          pip install virtualenv
          virtualenv venv 
          source venv/bin/activate
          pip install -r requirements.txt
          cd reddit/spiders
          scrapy crawl post -s CLOSESPIDER_ITEMCOUNT=1000 -o posts.json
          ls csv/
          python json_parser.py
          cd csv
          ls
          pwd
          cat *csv > reddit_top_posts.csv
          rm reddit_downloaded.csv


      - name: Connect s3 bucket
        uses: jakejarvis/s3-sync-action@master
        env:
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: 'ap-south-1'   # optional: defaults to us-east-1
          SOURCE_DIR: '/home/runner/work/reddit_scrapper/reddit_scrapper/reddit/spiders/csv'